{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c845ce5",
   "metadata": {},
   "source": [
    "### 2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following *metrics* below: provide your answers and, in your own words, *concisely explain your rationale for your answers.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003530d",
   "metadata": {},
   "source": [
    "1. **Accuracy** measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14924eb",
   "metadata": {},
   "source": [
    "accuracy is mostly using in weather prediction, pass or fail grading, ans spam eamil detection, they all have balanced answers like rain or no rain, pass or fail, and spam or not spam. a high accuracy means both answers are being classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e931bf",
   "metadata": {},
   "source": [
    "2. **Sensitivity** measures the proportion of actual positives that are correctly identified.\n",
    "\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a21cb",
   "metadata": {},
   "source": [
    "sensitivity is using on dedical diagnosis, fraud detection, and epidemic outbreak detection. the purpose of sensitivity is to correctly identify the proportion of the actual positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba874d",
   "metadata": {},
   "source": [
    "3. **Specificity** measures the proportion of actual negatives that are correctly identified.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cb790",
   "metadata": {},
   "source": [
    "specificity is using in cancer screening, drug testing, and allergy testing. the purpose of the specificity is to correctly identify the proportion of the actual negative cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a91a31",
   "metadata": {},
   "source": [
    "4. **Precision** measures the proportion of positive identifications that were actually correct.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ae0fa",
   "metadata": {},
   "source": [
    "precision is using in cancer diagnosis, criminal investigations, and emergency response systems. the purpose of the precision is close to sensitivity, which purpose of it is to correctly identify the propoetion of the actual positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd6eb3",
   "metadata": {},
   "source": [
    "chatbot summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060002af",
   "metadata": {},
   "source": [
    "Here's a summary of our session:  \n",
    "\n",
    "1. **Classification Decision Trees vs. Multiple Linear Regression**  \n",
    "   - **Decision Trees:** Used for classification tasks, they split the data into branches based on feature thresholds to classify outcomes into discrete categories.  \n",
    "   - **Multiple Linear Regression:** Used for regression tasks, it predicts continuous outcomes by finding the best-fit line based on the relationship between independent variables and the dependent variable.\n",
    "\n",
    "2. **Real-World Applications of Evaluation Metrics:**  \n",
    "   - **Accuracy:** Useful for balanced datasets, such as spam detection or pass/fail grading.  \n",
    "   - **Sensitivity:** Important when missing true positives has significant consequences, such as in medical diagnosis or fraud detection.  \n",
    "   - **Specificity:** Crucial when false positives are costly, like in cancer screening or environmental monitoring.  \n",
    "   - **Precision:** Valuable in scenarios where false positives lead to unnecessary costs or harm, such as in cybersecurity or hiring systems.\n",
    "\n",
    "3. **Evaluation Metric Definitions with Examples:**  \n",
    "   - **Accuracy:** Measures overall correctness in balanced datasets.  \n",
    "   - **Sensitivity:** Ensures most true positives are detected when missing positives is critical.  \n",
    "   - **Specificity:** Focuses on avoiding false positives by accurately identifying negatives.  \n",
    "   - **Precision:** Prioritizes the reliability of positive predictions, minimizing false positives.  \n",
    "\n",
    "Let me know if you'd like to revisit any topic or need additional details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650d7d6",
   "metadata": {},
   "source": [
    "### 4. Create an 80/20 split with 80% of the data as a training set *ab_reduced_noNaN_train* and 20% of the data testing set  *ab_reduced_noNaN_test* using either *df.sample(...)* as done in TUT or using *train_test_split(...)* as done in the previous HW, and report on how many observations there are in the training data set and the test data set.<br><br>Tell a ChatBot that you are about to fit a \"scikit-learn\" *DecisionTreeClassifier* model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree *clf* using only the *List Price* variable to predict whether or not a book is a hard cover or paper back book using a *max_depth* of *2*; finally use *tree.plot_tree(clf)* to explain what *predictions* are made based on *List Price* for the fitted *clf* model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6ca28",
   "metadata": {},
   "source": [
    "using the df.sample done in tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55232894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b103fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, \n",
    "                         columns=cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2ed654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n  San Jose, CA, 1993.\\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n  July-August 1995.\\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n  163-171.\\n\\n|details-end|',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the binary 'target' \n",
    "# with 'target_names': array(['malignant', 'benign'], dtype='<U9')\n",
    "cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c5298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set observations: 455\n",
      "Test set observations: 114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer_df['target'] = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer_df.drop(columns=['target']),  \n",
    "    cancer_df['target'],                \n",
    "    test_size=0.2,                      \n",
    "    random_state=42)\n",
    "\n",
    "print(f\"Training set observations: {X_train.shape[0]}\")\n",
    "print(f\"Test set observations: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e239389",
   "metadata": {},
   "source": [
    "we should use \"ab_reduced_noNaN_train\" datato initially fit the classification tree, becasue this is the data that the model will learn from. It contains both the list price and the target variable hard_or_paper. by fitting the model to this data so it can learn the relationship between the list price and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c38c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Features (X_train) Shape: (455, 30)\n",
      "Training Set Target (y_train) Shape: (455, 2)\n",
      "Test Set Features (X_test) Shape: (114, 30)\n",
      "Test Set Target (y_test) Shape: (114, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "\n",
    "# Include the target column\n",
    "cancer_df['target'] = cancer_data.target\n",
    "\n",
    "# 80/20 split of the data into training and testing sets\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(cancer_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you can extract the 'target_names' and perform one-hot encoding\n",
    "y_train = pd.get_dummies(ab_reduced_noNaN_train['target'], drop_first=False)  # One-hot encoding for target\n",
    "X_train = ab_reduced_noNaN_train[cancer_data.feature_names]  # Features for training\n",
    "\n",
    "# Similarly for the test set\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test['target'], drop_first=False)\n",
    "X_test = ab_reduced_noNaN_test[cancer_data.feature_names]\n",
    "\n",
    "# Output the shapes of the split datasets\n",
    "print(\"Training Set Features (X_train) Shape:\", X_train.shape)\n",
    "print(\"Training Set Target (y_train) Shape:\", y_train.shape)\n",
    "print(\"Test Set Features (X_test) Shape:\", X_test.shape)\n",
    "print(\"Test Set Target (y_test) Shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81419fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']\n",
    "#X = ab_reduced_noNaN[['List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79118",
   "metadata": {},
   "source": [
    "y is the binary target variable (1 for \"Hard\" and 0 for \"Paper\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6f58d",
   "metadata": {},
   "source": [
    "X is the feature matrix with the book's List Price as the input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35b9a94a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['List Price'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the feature (List Price) and the target (Hard_or_Paper)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mab_reduced_noNaN_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mList Price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Feature\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(ab_reduced_noNaN_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Target (one-hot encoded)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the DecisionTreeClassifier with max_depth=2\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['List Price'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the feature (List Price) and the target (Hard_or_Paper)\n",
    "X_train = ab_reduced_noNaN_train[['List Price']]  # Feature\n",
    "y_train = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']  # Target (one-hot encoded)\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Train the classifier (fit the model)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# If you want to visualize the tree, you can use this:\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True, max_depth=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6340d",
   "metadata": {},
   "source": [
    "chatbot summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71436801",
   "metadata": {},
   "source": [
    "Here's a summary of our session:\n",
    "\n",
    "1. **Data Splitting for Breast Cancer Dataset**:\n",
    "   - We discussed how to split the `cancer_df` (based on `datasets.load_breast_cancer()` from `sklearn`) into an 80% training set and 20% testing set using `train_test_split`.\n",
    "   - Explained how to perform one-hot encoding for the `target` variable using `pd.get_dummies`.\n",
    "\n",
    "2. **Training a Decision Tree Classifier**:\n",
    "   - Provided code to train a `DecisionTreeClassifier` on a single feature (`List Price`) to predict whether a book is hardcover or paperback.\n",
    "   - Specified the use of `max_depth=2` to constrain the tree's complexity.\n",
    "   - Included evaluation of training accuracy and optional visualization of the decision tree.\n",
    "\n",
    "3. **Debugging Column Not Found Error**:\n",
    "   - Identified the error `None of [Index(['List Price'], dtype='object')] are in the [columns]` as a mismatch in column names.\n",
    "   - Suggested checking the column names with `print(columns)`.\n",
    "   - Provided solutions to handle potential issues such as leading/trailing spaces, case sensitivity, or column renaming.\n",
    "\n",
    "Let me know if there are any additional topics you'd like to revisit or expand upon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de4a29",
   "metadata": {},
   "source": [
    "### 6. Use previously created *ab_reduced_noNaN_test* to create confusion matrices for *clf* and *clf2*. Report the sensitivity, specificity and accuracy for each of the models<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ff01cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['List Price'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Extract the test feature (List Price) and target (Hard_or_Paper)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mab_reduced_noNaN_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mList Price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(ab_reduced_noNaN_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# One-hot encoding\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predictions for clf and clf2\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['List Price'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Extract the test feature (List Price) and target (Hard_or_Paper)\n",
    "X_test = ab_reduced_noNaN_test[['List Price']]\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']  # One-hot encoding\n",
    "\n",
    "# Predictions for clf and clf2\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "y_pred_clf2 = clf2.predict(X_test)\n",
    "\n",
    "# Confusion Matrices\n",
    "conf_matrix_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "conf_matrix_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "# Sensitivity, Specificity, and Accuracy for clf\n",
    "tn_clf, fp_clf, fn_clf, tp_clf = conf_matrix_clf.ravel()\n",
    "sensitivity_clf = tp_clf / (tp_clf + fn_clf)  # True Positive Rate\n",
    "specificity_clf = tn_clf / (tn_clf + fp_clf)  # True Negative Rate\n",
    "accuracy_clf = accuracy_score(y_test, y_pred_clf)\n",
    "\n",
    "# Sensitivity, Specificity, and Accuracy for clf2\n",
    "tn_clf2, fp_clf2, fn_clf2, tp_clf2 = conf_matrix_clf2.ravel()\n",
    "sensitivity_clf2 = tp_clf2 / (tp_clf2 + fn_clf2)  # True Positive Rate\n",
    "specificity_clf2 = tn_clf2 / (tn_clf2 + fp_clf2)  # True Negative Rate\n",
    "accuracy_clf2 = accuracy_score(y_test, y_pred_clf2)\n",
    "\n",
    "# Report results\n",
    "print(\"Confusion Matrix for clf:\")\n",
    "print(conf_matrix_clf)\n",
    "print(f\"Sensitivity (clf): {sensitivity_clf:.2f}\")\n",
    "print(f\"Specificity (clf): {specificity_clf:.2f}\")\n",
    "print(f\"Accuracy (clf): {accuracy_clf:.2f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix for clf2:\")\n",
    "print(conf_matrix_clf2)\n",
    "print(f\"Sensitivity (clf2): {sensitivity_clf2:.2f}\")\n",
    "print(f\"Specificity (clf2): {specificity_clf2:.2f}\")\n",
    "print(f\"Accuracy (clf2): {accuracy_clf2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52767257",
   "metadata": {},
   "source": [
    "chatbot summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6934e",
   "metadata": {},
   "source": [
    "Here’s a summary of our current session:\n",
    "\n",
    "1. **Training a Decision Tree Classifier**:\n",
    "   - Discussed training a `DecisionTreeClassifier` (`clf`) using the `List Price` variable to predict whether a book is hardcover or paperback. \n",
    "   - The model was limited to a `max_depth=2` to constrain complexity.\n",
    "\n",
    "2. **Debugging Column Not Found Error**:\n",
    "   - Identified and resolved an issue where the column `List Price` was not found in the dataset due to potential name mismatches, spaces, or case differences.\n",
    "   - Suggested approaches to handle these issues, including renaming columns, stripping spaces, and standardizing case.\n",
    "\n",
    "3. **Evaluating Models with Confusion Matrices**:\n",
    "   - Provided code to compute confusion matrices for two classifiers, `clf` and `clf2`, on the `ab_reduced_noNaN_test` dataset.\n",
    "   - Calculated sensitivity, specificity, and accuracy for both models to assess performance.\n",
    "   - Explained how to interpret confusion matrix results and the importance of these metrics.\n",
    "\n",
    "Let me know if you’d like to revisit any part or explore additional topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69769c",
   "metadata": {},
   "source": [
    "### 7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for *clf* and *clf2*) are better<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05858",
   "metadata": {},
   "source": [
    "```python\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(ab_reduced_noNaN_train[['List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(\n",
    "                         ab_reduced_noNaN_train[['NumPages','Thick','List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17fb71",
   "metadata": {},
   "source": [
    "the difference between the two matrices is the input features used by the classifier models. in the first matrix, the predictions are only based on the list price, which might cause high misclassification rate due to limited information. and in the second matrix, the classifier uses more features, which provides more information that is improving the prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c8561",
   "metadata": {},
   "source": [
    "the clf and clf2 are better because they leverage additional features, which leads to the models with improved understanding of the relationships between the input data and the target outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4240fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
